{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d91ccb81-7980-4de1-9346-38dda696fac2",
   "metadata": {},
   "source": [
    "# Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56bbe17-ddd4-4c4e-8c9a-7fbc03bbfeec",
   "metadata": {},
   "source": [
    "Web scraping refers to the extraction of data from a website. \n",
    "This information is collected and then exported into a format that is more useful for the user. \n",
    "Be it a spreadsheet or an API.\n",
    "\n",
    "Although web scraping can be done manually, in most cases, automated tools are preferred when scraping web data as they can be less costly and work at a faster rate.\n",
    "\n",
    "But in most cases, web scraping is not a simple task. Websites come in many shapes and forms, as a result, web scrapers vary in functionality and features.\n",
    "\n",
    "\n",
    "Also known as web data extraction and web harvesting, web scraping is the process of extracting data from a website. While you can do this manually, when projects require extracted data from hundreds or even thousands of web pages, automated web scraping tools can do the job more quickly and efficiently. \n",
    "\n",
    "Web scraping tools collect and export extracted data for in-depth analysis, typically into a central local database, spreadsheet, or API. \n",
    "\n",
    "Web scraping software may access the internet either through HTTP or a web browser, with the web crawler and web scraper working together to extract specific data from the web pages. We’ll discuss web crawlers and web scrapers in greater detail later in this article.\n",
    "\n",
    "Before data extraction can take place, it must fetch the webpage. Fetching refers to the process of downloading a web page. The browser does this every time a user visits a web page. The web page’s content is then parsed (i.e., analyzed for syntax), reformatted, or searched, with the extracted data then loaded into a database or copied into a spreadsheet.\n",
    "\n",
    "\n",
    "The web scraping tool makes HTTP requests to the website and then extracts data from the web pages. It parses publicly available content, which the server renders as HTML. \n",
    "\n",
    "It also requests internal APIs for related data, such as product prices and SKUs. A database stores this data and delivers it to the browser through HTTP requests.  \n",
    "\n",
    "Successful web data extraction requires the work of two components: the web crawler and the web scraper. The crawler guides the scraper throughout the web pages and extracts the requested data.\n",
    "\n",
    "Web scraping is the process of collecting structured web data in an automated manner. It’s also widely known as web data extraction or web data scraping.\n",
    "\n",
    "Some of the main use cases of web scraping include price monitoring, price intelligence, news monitoring, lead generation, and market research among many others.\n",
    "\n",
    "In general, web scraping is used by people and businesses who want to make use of publicly available web data to generate valuable insights and make smarter decisions.\n",
    "\n",
    "\n",
    "Data scraping has numerous applications across many industries—including insurance, banking, finance, trading, eCommerce, sports, and digital marketing. Data is also used to inform decision-making, generate leads and sales, manage risks, guide strategies, and create new products and services.\n",
    "\n",
    "Price Intelligence\n",
    "Price intelligence refers to monitoring a competitor’s prices and responding to their changes in pricing. Retailers use price intelligence to maintain a competitive edge over their rivals.\n",
    "\n",
    "Effective price intelligence involves web scraping, with eCommerce sellers extracting product and pricing information from other eCommerce websites to guide their pricing and marketing decisions.\n",
    "\n",
    "Price intelligence remains one of the most prominent use cases for web scraping due to valuable data for revenue optimization, product trend monitoring, dynamic pricing, competitor monitoring, and other applications. \n",
    "\n",
    "Market Research\n",
    "Web data extraction plays a vital role in market research. Market researchers use the resulting data to inform their market trend analysis, research and development, competitor analysis, price analysis, and other areas of study.\n",
    "\n",
    "Lead Generation\n",
    "Businesses that want to attract new customers and generate more sales need to launch effective sales and marketing campaigns. Web scraping can help companies gather the correct contact information from their target market—including names, job titles, email addresses, and cellphone numbers. Then, they can reach out to these contacts and generate more leads and sales for their business. \n",
    "\n",
    "Brand Monitoring\n",
    "Brands increasingly use social listening and monitoring tools to gauge the public’s perception of their brands. You can use web scraping software to extract real-time data from various sources (including social media platforms and review sites). You can then analyze the aggregated data to gauge brand sentiment. \n",
    "\n",
    "Business Automation\n",
    "In some cases, you may need to extract large amounts of data from a group of websites. You need to do this consistently, quickly, and structured. You can use web scraping tools to automatically extract these data sets. \n",
    "\n",
    "Real Estate\n",
    "You need web data extraction to generate the most up-to-date and accurate real estate listings. Web scraping is commonly used to retrieve the most updated data about properties, sale prices, monthly rental income, amenities, property agents, and other data points.\n",
    "\n",
    "Web scraped data also informs property value appraisals, rental yield estimates, and real estate market trends analysis. \n",
    "\n",
    "Alternative Data for Finance\n",
    "Web-scraped data is increasingly harnessed by investors to inform their trades and strategies. Use cases include: extracting insights from SEC filings, monitoring news and stock market performance, public sentiment integrations, and extracting stock market data from Yahoo Finance.\n",
    "\n",
    "News and Content Marketing\n",
    "Businesses, political campaigns, and nonprofits that need to keep a close eye on brand sentiment, polls, and other trends often invest in web scraping tools. Content and digital marketing agencies also use web scraping tools to monitor, aggregate, and parse the most critical stories from different industries. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d24df3-959f-491e-982d-115a3d67236f",
   "metadata": {},
   "source": [
    "# Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f135aa57-c4fc-49f0-9ed7-76a893858384",
   "metadata": {},
   "source": [
    "Data Scraping Techniques:\n",
    "Here are a few techniques commonly used to scrape data from websites. In general, all web scraping techniques retrieve content from websites, process it using a scraping engine, and generate one or more data files with the extracted content.\n",
    "\n",
    "HTML Parsing\n",
    "HTML parsing involves the use of JavaScript to target a linear or nested HTML page. It is a powerful and fast method for extracting text and links (e.g. a nested link or email address), scraping screens and pulling resources.\n",
    "\n",
    "DOM Parsing\n",
    "The Document Object Model (DOM) defines the structure, style and content of an XML file. Scrapers typically use a DOM parser to view the structure of web pages in depth. DOM parsers can be used to access the nodes that contain information and scrape the web page with tools like XPath. For dynamically generated content, scrapers can embed web browsers like Firefox and Internet Explorer to extract whole web pages (or parts of them).\n",
    "\n",
    "Vertical Aggregation\n",
    "Companies that use extensive computing power can create vertical aggregation platforms to target particular verticals. These are data harvesting platforms that can be run on the cloud and are used to automatically generate and monitor bots for certain verticals with minimal human intervention. Bots are generated according to the information required to each vertical, and their efficiency is determined by the quality of data they extract.\n",
    "\n",
    "XPath\n",
    "XPath is short for XML Path Language, which is a query language for XML documents. XML documents have tree-like structures, so scrapers can use XPath to navigate through them by selecting nodes according to various parameters. A scraper may combine DOM parsing with XPath to extract whole web pages and publish them on a destination site.\n",
    "\n",
    "Google Sheets\n",
    "Google Sheets is a popular tool for data scraping. Scarpers can use the IMPORTXML function in Sheets to scrape from a website, which is useful if they want to extract a specific pattern or data from the website. This command also makes it possible to check if a website can be scraped or is protected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44118f7-c965-4919-808c-74198373c35c",
   "metadata": {},
   "source": [
    "# Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189090ce-6860-4b7a-8538-97aff574ce7a",
   "metadata": {},
   "source": [
    "Beautiful Soup is a python package which allows us to pull data out of HTML and XML documents.\n",
    "\n",
    "Beautiful Soup is a great tool for extracting very specific information from large unstructured raw Data, and also it is very fast and handy to use.\n",
    "\n",
    "Beautiful Soup is a Python library for getting data out of HTML, XML, and other markup languages.\n",
    "Say you’ve found some webpages that display data relevant to your research, such as date or address information, but that do not provide any way of downloading the data directly. Beautiful Soup helps you pull particular content from a webpage, remove the HTML markup, and save the information. It is a tool for web scraping that helps you clean up and parse the documents you have pulled down from the web.\n",
    "\n",
    "Beautiful Soup is a Python package for parsing HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup). It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping.\n",
    "\n",
    "Beautiful Soup was started by Leonard Richardson, who continues to contribute to the project and is additionally supported by Tidelift, a paid subscription to open-source maintenance.\n",
    "\n",
    "Beautiful Soup provides simple methods for navigating, searching, and modifying a parse tree in HTML, XML files. It transforms a complex HTML document into a tree of Python objects. It also automatically converts the document to Unicode, so you don’t have to think about encodings. This tool not only helps you scrape but also to clean the data. Beautiful Soup supports the HTML parser included in Python’s standard library, but it also supports several third-party Python parsers like lxml or hml5lib.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49083c3b-afc3-45ae-8f2c-706642f2b188",
   "metadata": {},
   "source": [
    "# Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978e4468-3bf4-47e5-ace6-0833b48702cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flask is used for developing web applications using python, implemented on Werkzeug and Jinja2. \n",
    "\n",
    "Advantages of using Flask framework are:\n",
    "There is a built-in development server and a fast debugger provided.\n",
    "Lightweight\n",
    "Secure cookies are supported.\n",
    "Templating using Jinja2.\n",
    "Request dispatching using REST.\n",
    "Support for unit testing is built-in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b5bd54-ad17-4c05-88b5-c2f98ae4867e",
   "metadata": {},
   "source": [
    "# Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3178720-565d-4a95-b168-20d9aa2fd116",
   "metadata": {},
   "source": [
    "AWS (Amazon Web Services) is the largest cloud computing platform with over 200+ featured resources. It is a platform that provides a pay-as-you-go service.\n",
    "\n",
    "AWS offers a “pay-as-you-go” feature where you just have to pay for the services you use and also the time period you use.\n",
    "\n",
    "\n",
    "Amazon Elastic Beanstalk:\n",
    "Amazon Elastic Beanstalk is an AWS service used for deployment and scaling web applications developed using Java, PHP, Python, Docker, etc. It supports running and managing web applications. You just need to upload your code and the deployment part is handled by Elastic Beanstalk (from capacity provisioning, load balancing, and auto-scaling to application health monitoring). It is the best service for developers since it takes care of the servers, load balancers, and firewalls. Also, you can have control over AWS assets and the other resources required for the application. You get the benefit of paying for what you use, thus maintaining cost-effectiveness.  \n",
    "\n",
    "\n",
    "AWS CodePipeline:\n",
    "AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.\n",
    "\n",
    "AWS CodePipeline is a continuous delivery service that enables you to model, visualize, and automate the steps required to release your software. With AWS CodePipeline, you model the full release process for building your code, deploying to pre-production environments, testing your application and releasing it to production. AWS CodePipeline then builds, tests, and deploys your application according to the defined workflow every time there is a code change. You can integrate partner tools and your own custom tools into any stage of the release process to form an end-to-end continuous delivery solution.\n",
    "\n",
    "By automating your build, test, and release processes, AWS CodePipeline enables you to increase the speed and quality of your software updates by running all new changes through a consistent set of quality checks.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
